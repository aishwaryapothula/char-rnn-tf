{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 1115394 characters of which 65 are unique.\n"
     ]
    }
   ],
   "source": [
    "# Data I/O\n",
    "# open() opens the 'input.txt' in read mode and read() reads the data into 'data'\n",
    "# set() returns unique characters by eliminating repetitions\n",
    "# list() returns a list of characters\n",
    "# sorted() sorts the characters in an alphabetical order\n",
    "# len() gives the number of characters\n",
    "# enumerate() associated a number with each character\n",
    "\n",
    "#file = sys.argv[1]\n",
    "data = open(\"input.txt\", 'r').read()\n",
    "chars = sorted(list(set(data)))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('Data has %d characters of which %d are unique.' % (data_size, vocab_size))\n",
    "# creating dictionaries with key:value as char:number and vice-versa\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot representation returns an identity matrix of dimension v x vocab_size\n",
    "# first array is the dimension of the identity matrix, Second array selects the one-hot row corresponding to each label\n",
    "\n",
    "def one_hot(v):\n",
    "    return np.eye(vocab_size)[v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "# hidden_size set to 100 specifying number of nodes in hidden layer\n",
    "# seq_length specifies the windoe length while traversing the data from left to right\n",
    "# window length specifies that 'short term memory' for the RNN\n",
    "# Learning rate is the gradient step and is usually kept as 1e-1\n",
    "\n",
    "hidden_size = 100  \n",
    "seq_length = 25   \n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed value \n",
    "# seed value is set so that random values are generated starting from the seed value everytime the model is run(to replicate)\n",
    "\n",
    "seed_value = 42\n",
    "tf.set_random_seed(seed_value)\n",
    "random.seed(seed_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor placeholders\n",
    "# tf.placeholder() takes the arguments shape, datatype to create tensor type placeholders for all data to be passed in the model \n",
    "# data is fed into the placeholders later\n",
    "# an object called initializer is created\n",
    "# tf.random_normal_initializer(stddev=0.1) generated random normalized data with stdev 0.1\n",
    "\n",
    "inputs = tf.placeholder(shape=[None, vocab_size],dtype=tf.float32, name=\"inputs\")\n",
    "targets = tf.placeholder(shape=[None, vocab_size], dtype=tf.float32, name=\"targets\")\n",
    "init_state = tf.placeholder(shape=[1, hidden_size], dtype=tf.float32, name=\"state\")\n",
    "initializer = tf.random_normal_initializer(stddev=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-011ac49a095f>:29: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "[(<tf.Tensor 'gradients/AddN_27:0' shape=(65, 100) dtype=float32>, <tf.Variable 'RNN/Wxh:0' shape=(65, 100) dtype=float32_ref>), (<tf.Tensor 'gradients/AddN_28:0' shape=(100, 100) dtype=float32>, <tf.Variable 'RNN/Whh:0' shape=(100, 100) dtype=float32_ref>), (<tf.Tensor 'gradients/AddN_1:0' shape=(100, 65) dtype=float32>, <tf.Variable 'RNN/Why:0' shape=(100, 65) dtype=float32_ref>), (<tf.Tensor 'gradients/AddN_26:0' shape=(100,) dtype=float32>, <tf.Variable 'RNN/bh:0' shape=(100,) dtype=float32_ref>), (<tf.Tensor 'gradients/AddN:0' shape=(65,) dtype=float32>, <tf.Variable 'RNN/by:0' shape=(65,) dtype=float32_ref>)]\n",
      "iter: 0, p: 0, loss: 4.216357\n",
      "----\n",
      " CCctZqqjNbViVuYtsN swq!GuoFmoqgNuoXuAVuzP3xL:RpaZpAXTnpYlgN BvfdZ,rpHIWaO'IpEsozY zCXA$!&XEnZV&$ kRPg pAadrVk3.,o&VoY&u3uCwO3.eWpn!!doopupYSSIgpN!&eEq'pN&:t'WuDl3Ld&ZpDokKsF!p,:PVzz.pvojSoOSa!t.LNaFmm \n",
      "----\n",
      "\n",
      "iter: 500, p: 12500, loss: 2.635253\n",
      "----\n",
      " \n",
      " &IIIESUpSfse nht'se gou a,snisg hor Oh nhe tnt \n",
      "ir tlieees ahu convaom Tatk  eds,s Aeonir le wonc fs zCpdhol ao  ha,w leen: iiRr fIdnfsB\n",
      "\n",
      "WAm cccn caiu coe znvec\n",
      "thgeZ bTse Cannn hsttuu dans boiecb: \n",
      "----\n",
      "\n",
      "iter: 1000, p: 25000, loss: 2.349797\n",
      "----\n",
      " owaw  hets sercenna, T;oan whowathenf,\n",
      "\n",
      "As fouriso sart wr turgenk\n",
      "arct\n",
      "MO: wall hem rfs.\n",
      "sor hen bethend he wr bo, anr wery huoodeuit mor berfeld,\n",
      "\n",
      "ir ;onn:eds Wisty unhg fof hesd sas,  hudt :es \n",
      "arn \n",
      "----\n",
      "\n",
      "iter: 1500, p: 37500, loss: 2.755265\n",
      "----\n",
      " wllle toul ont y b of iit tis youranlwyev il ty!u tot on aro'\n",
      ";\n",
      "itour leos thr'pdnovith w hing,ethet\n",
      "un tue\n",
      "ge tf yo s at th beafagdire sinkg\n",
      "Bb li:\n",
      "\n",
      ", ar thinof avvits wivep Ehew bn aon lusd tenre th \n",
      "----\n",
      "\n",
      "iter: 2000, p: 50000, loss: 2.241675\n",
      "----\n",
      " n; dlec falitr\n",
      "Bem, ioveh resveyd re ouus rorarehis\n",
      "TutyifrMcont, mo thera baes ao n'of\n",
      "sede thelis the hof ho dodeocHel\n",
      "\n",
      "HacK tf ande nd tolt,\n",
      "To pio le po ms eat hatt nt anestte atd ishe\n",
      "\n",
      "Sathe,\n",
      "Ohe \n",
      "----\n",
      "\n",
      "iter: 2500, p: 62500, loss: 1.571785\n",
      "----\n",
      " c--d.\n",
      "H\n",
      "Shof minlt ha cimd sena:\n",
      "Wthe m igct wim' ns orsed, be chisk githe plour;mava d sid bly hes locg, so poath y voredse?\n",
      "\n",
      "NappseTcheve veist our thel bunger, ve rive nood your cormus, yo blenowls \n",
      "----\n",
      "\n",
      "iter: 3000, p: 75000, loss: 1.755196\n",
      "----\n",
      "  dibe ceed lit the watus aned thee sed wett the wis,\n",
      "Lot thampeeit,\n",
      "Th d soltore.\n",
      "\n",
      "CORINLAIUS:\n",
      "Woce segt.\n",
      "\n",
      "CEGIOLAAUS: whtine, bed.\n",
      "\n",
      "MINZUI: l'me thetDe--\n",
      "Was an. pTith witdev so nomantret pow thei ho \n",
      "----\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-011ac49a095f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     hprev_val, loss_val, _ = sess.run([hprev, loss, updates],\n\u001b[0;32m---> 72\u001b[0;31m                                       feed_dict={inputs: input_vals,targets: target_vals,init_state: hprev_val})\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# weights and biases initialization\n",
    "# tf.variable_scope() defines the scope for the variables defined inside it\n",
    "# hs_t is the hidden state and a placeholder is created\n",
    "# ys is list that stores the target tensors of the output variable\n",
    "# get_variable() retrives a variable from outside of scope to inside of scope\n",
    "# weights and biases are initialzed to random normal values with stdev of 0.1\n",
    "# xh-input ot hidden hh-prev hidden layer to next hy-hidden layer to output layer\n",
    "# we use tanh activation function for all layers except output. softmax for output\n",
    "# ys_t is the output state tensor\n",
    "\n",
    "with tf.variable_scope(\"RNN\") as scope:\n",
    "    hs_t = init_state\n",
    "    ys = []\n",
    "    for t, xs_t in enumerate(tf.split(inputs, seq_length, axis=0)):\n",
    "        if t > 0:\n",
    "            scope.reuse_variables()  # Reuse variables\n",
    "        Wxh = tf.get_variable(\"Wxh\", [vocab_size, hidden_size], initializer=initializer)\n",
    "        Whh = tf.get_variable(\"Whh\", [hidden_size, hidden_size], initializer=initializer)\n",
    "        Why = tf.get_variable(\"Why\", [hidden_size, vocab_size], initializer=initializer)\n",
    "        bh = tf.get_variable(\"bh\", [hidden_size], initializer=initializer)\n",
    "        by = tf.get_variable(\"by\", [vocab_size], initializer=initializer)\n",
    "\n",
    "        hs_t = tf.tanh(tf.matmul(xs_t, Wxh) + tf.matmul(hs_t, Whh) + bh)\n",
    "        ys_t = tf.matmul(hs_t, Why) + by\n",
    "        ys.append(ys_t)\n",
    "        \n",
    "# softmax, loss function, and optimizer\n",
    "# after the input layer, hprev is updated to output of previous layer hs_t\n",
    "# softmax is applied to output layer to get output probabilities\n",
    "# reduce_mean calculates the mean\n",
    "# cross_entropy calculated the difference between target and inferred output probability distributions\n",
    "# logits deals with un-normalized values\n",
    "\n",
    "hprev = hs_t\n",
    "output_softmax = tf.nn.softmax(ys[-1])  # Get softmax for sampling\n",
    "\n",
    "outputs = tf.concat(ys, axis=0)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=targets, logits=outputs))\n",
    "\n",
    "# Minimizer\n",
    "# Adamoptimizer is an extension of the stochastic gradient, it maintains and changes learning_rate for each variable\n",
    "# compute_gradients() computes the gradients, return values of variable,gradient pair\n",
    "\n",
    "minimizer = tf.train.AdamOptimizer()\n",
    "grads_and_vars = minimizer.compute_gradients(loss)\n",
    "print(grads_and_vars)\n",
    "\n",
    "# Gradient Clipping\n",
    "# Rnns face an issue of exploding gradients as there are repeated matric multiplications\n",
    "# gradient clipping is used to deal with exploding gradients\n",
    "# gradient clipping clips graient between negative and positve of specified value\n",
    "\n",
    "grad_clipping = tf.constant(5.0, name=\"grad_clipping\")\n",
    "clipped_grads_and_vars = []\n",
    "for grad, var in grads_and_vars:\n",
    "    clipped_grad = tf.clip_by_value(grad, -grad_clipping, grad_clipping)\n",
    "    clipped_grads_and_vars.append((clipped_grad, var))\n",
    "\n",
    "# Gradient updates\n",
    "updates = minimizer.apply_gradients(clipped_grads_and_vars)\n",
    "\n",
    "# Session\n",
    "# tf.session() allocates resources for running\n",
    "# tf.global_variables_initializer() initializes all the tensor types variables\n",
    "# session.run() starts running the process\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "writer=tf.summary.FileWriter(\"visual\",sess.graph)\n",
    "sess.run(init)\n",
    "writer.close()\n",
    "\n",
    "# Initial values\n",
    "# n counts the number of iterations, p is a datapointer\n",
    "# hprev_val set to zeros of dimensions of [1,hidden-size]\n",
    "\n",
    "n, p = 0, 0\n",
    "hprev_val = np.zeros([1, hidden_size])\n",
    "\n",
    "# moving and sampling\n",
    "# p is initialized to 0 so data[p:p + seq_length] slices the data from [0:0+25]\n",
    "# data[p + 1:p + seq_length + 1] slices data from [1:26]\n",
    "# after the data is sliced for input_vals, the window moves by one character and the data for target_vals is sliced inorder for them both to be mapped\n",
    "# One-hot encoding of the input and output vals is created\n",
    "# feed_dict is a parameter of the method tf.session.run() and the outputs are stored accordingly\n",
    "# data wont be sampled untill 500th iteration\n",
    "# sample length is specified \n",
    "# to randomly select the starting point of the sampling data random.randint(0, len(data) - seq_length)\n",
    "# [char_to_ix[ch] for ch in data[start_ix:start_ix + seq_length]] iterating over data[start point:start+seq_length],identifying the associated number with the char in the dictionary and storing values in sample_seq_ix \n",
    "# When you pick a random sample, it has different probabilities; to make it have a a probability distribution similar to that of data argument p is set from the softmax output of the sample\n",
    "\n",
    "tf.global_variables_initializer()\n",
    "while True:\n",
    "    # Initialize\n",
    "    if p + seq_length+1 >= len(data) or n == 0:\n",
    "        hprev_val = np.zeros([1, hidden_size])\n",
    "        p = 0  # reset\n",
    "\n",
    "    # Prepare inputs\n",
    "    input_vals = [char_to_ix[ch] for ch in data[p:p + seq_length]]\n",
    "    target_vals = [char_to_ix[ch] for ch in data[p + 1:p + seq_length + 1]]\n",
    "\n",
    "    input_vals = one_hot(input_vals)\n",
    "    target_vals = one_hot(target_vals)\n",
    "\n",
    "    hprev_val, loss_val, _ = sess.run([hprev, loss, updates],\n",
    "                                      feed_dict={inputs: input_vals,targets: target_vals,init_state: hprev_val})\n",
    "    if n % 500 == 0:\n",
    "        # Progress\n",
    "        print('iter: %d, p: %d, loss: %f' % (n, p, loss_val))\n",
    "\n",
    "        # Do sampling\n",
    "        sample_length = 200\n",
    "        start_ix = random.randint(0, len(data) - seq_length)\n",
    "        sample_seq_ix = [char_to_ix[ch] for ch in data[start_ix:start_ix + seq_length]]\n",
    "        ixes = []\n",
    "        sample_prev_state_val = np.copy(hprev_val)\n",
    "\n",
    "        for t in range(sample_length):\n",
    "            sample_input_vals = one_hot(sample_seq_ix)\n",
    "            sample_output_softmax_val, sample_prev_state_val = \\\n",
    "                sess.run([output_softmax, hprev],\n",
    "                         feed_dict={inputs: sample_input_vals,init_state: sample_prev_state_val})\n",
    "\n",
    "            ix = np.random.choice(range(vocab_size), p=sample_output_softmax_val.ravel())\n",
    "            ixes.append(ix)\n",
    "            sample_seq_ix = sample_seq_ix[1:] + [ix]\n",
    "\n",
    "        txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
    "        print('----\\n %s \\n----\\n' % (txt,))\n",
    "\n",
    "    p += seq_length\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
